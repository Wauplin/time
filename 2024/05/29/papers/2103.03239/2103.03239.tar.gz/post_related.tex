\section{Additional Related Work}
\label{sect:post_related}

In this section, we review some of the papers relevant to our work, but omitted from the main part due to space constraints. 

\subsection{Decentralized training}\label{sect:post_related_gossip}
In this subsection, we give additional details about the dependence of gossip-based optimization methods on the spectral properties on the communication graph through the spectral properties of the mixing matrix~\cite{xiao2004fast,scaman2019optimal} or the Laplacian matrix~\cite{merris1994laplacian,uribe2020dual} of the network. 
That is, gossip finds approximate average on nodes with accuracy $\varepsilon$ after $\cO\left((1-\lambda_2(\mM))^{-1}\log(\varepsilon^{-1})\right)$ iterations, where $\mM$ is the mixing matrix and $\lambda_2(\mM)$ is the second largest eigenvalue of $\mM$ when sorted by absolute value. 
The quantity $\eta = 1-\lambda_2(\mM)$ is called the spectral gap of the mixing matrix $\mM$, and $\eta^{-1}$ is typically a polynomial of the total number of nodes $N$ when the maximal degree of the node is $\cO(1)$. For example, for uniformly averaging $\mM$ one can show that $\eta^{-1} = \cO(N^2)$ for the ring topology (node degree $2$), $\eta^{-1} = \cO(N)$ for the two-dimensional torus topology (node degree  $2$), and $\eta^{-1} = \cO(1)$ for the fully connected graph (node degree $N-1$); one can find more examples in~\cite{aldous2002reversible}. Similarly, the communication complexity of decentralized optimization methods often has multiplicative dependence on either $\cO(\eta^{-1})$ (see~\cite{xu2020distributed} and references therein) or $\cO(\eta^{-\nicefrac{1}{2}})$~\cite{scaman2019optimal,uribe2020dual,fallah2019robust,kovalev2020optimal}, which is not improvable for gossip-based methods~\cite{arjevani2015communication,scaman2017optimal}.

Contrary to this, Moshpit All-Reduce does not depend on a fixed communication graph and the properties of its mixing matrix.
However, it depends on the number of averaging groups and the total number of peers (see Theorem~\ref{thm:quality_of_avg_deterministic_vectors}), which can be viewed as properties of a time-varying random communication graph. Fortunately, this dependence is often much better than in gossip: as we mentioned in the main part of the paper, even if workers are randomly split into pairs at each iteration, the simplified version of Moshpit All-Reduce makes the average distortion (the left-hand side of Equation~\ref{eq:determ_quality_of_avg}) at least $2$ times smaller after each round on average.

\subsection{Compressed communication}
Another popular approach to address the communication bottleneck is communication compression~\cite{seide20141,alistarh2017qsgd,suresh2017distributed, ramezani2021nuqsgd, faghri2020adaptive}: before sending any information (e.g., iterates, gradients, Hessians or more sophisticated data) over the network, peers compress this information by applying a possibly random transformation. As the result, peers send fewer bits for each communication round, but the total number of communication rounds needed to achieve the predefined accuracy of the solution increases. However, compression can be useful in situations when the reduction in communication costs of one round is more important than the increase in the number of these rounds~\cite{horvath2019natural}.

There are two distinct groups of works on distributed training with compressed communication: ones that focus on unbiased compression operators (e.g., Rand-K, $\ell_p$-quantization) and ones studying algorithms with biased compressors (e.g., Top-K); see a detailed summary of  popular compression operators in~\cite{beznosikov2020biased}). 
Quantized SGD (QSGD)~\cite{alistarh2017qsgd} and TernGrad~\cite{wen2017terngrad} were among the first compression methods with convergence guarantees. Next, the convergence analysis of these methods was generalized and tightened in the (strongly) convex case in~\cite{mishchenko2019distributed}. Moreover, the authors of \cite{mishchenko2019distributed} proposed a modification of QSGD called DIANA: this algorithm is based on the quantization of gradients' differences, which helps it achieve linear convergence in the strongly convex case when peers compute full gradients. Next, DIANA was generalized to arbitrary unbiased compression in~\cite{horvath2019stochastic}, where authors also developed and analyzed the variance-reduced version of DIANA. After that, several further modifications, such as Accelerated DIANA~\cite{li2020acceleration} and DIANA with bidirectional compression~\cite{gorbunov2020linearly,philippenko2020artemis}, were proposed. Finally, we refer the reader to~\cite{li2020unified,haddadpour2020federated,das2020improved, pmlr-v139-gorbunov21a} for state-of-the-art results for distributed methods with unbiased compression in the non-convex case.

However, na√Øve application of biased compression operators can lead to significantly worse performance in practice. For instance, as it was shown recently in~\cite{beznosikov2020biased}, parallel SGD with Top-1 compression can diverge exponentially fast. Therefore, biased compressors are used jointly with so-called error-compensation~\cite{seide20141}. The first analysis of Error-Compensated SGD (EC-SGD) was proposed in~\cite{stich2018sparsified,karimireddy2019error} which then was generalized and tightened in~\cite{beznosikov2020biased}. Next, several further improvements, such as an accelerated version of EC-SGD~\cite{qian2020error} and linearly converging EC-SGD~\cite{gorbunov2020linearly}, were recently proposed. However, current theory does not show any superiority of distributed methods with biased compressors to the ones with unbiased compression operators.
In addition, one can combine decentralized communication with compression. Such combinations with unbiased compression operators were studied in~\cite{reisizadeh2019exact,kovalev2020linearly} and with biased operators in~\cite{pmlr-v97-koloskova19a,Koloskova2020Decentralized}.
In this paper, we do not study the interaction of different compression methods and Moshpit Averaging, leaving this promising direction to future work.

\subsection{Multiple local steps}
Alternatively, to reduce the impact of the communication bottleneck, it is possible to perform several local optimization steps on each peer between the communication rounds.
This approach is based on the idea that the increased computational load of peers will decrease the number of communication rounds required to obtain the optimal parameters; it is frequently used in federated learning~\cite{konevcny2016federated,kairouz2019advances}. In particular, one of the most popular methods with multiple local steps is called Local-SGD or Federated Averaging~\cite{konevcny2016federated,Stich18local}. The first results on its convergence were given in \cite{Stich18local,LinSPJ2018local}, and later they were tightened and generalized both for homogeneous~\cite{khaled2020tighter,woodworth2020local} and heterogeneous  cases~\cite{khaled2020tighter,woodworth2020minibatch}. Recently, further modifications of Local-SGD were proposed and analyzed: these modifications include acceleration \cite{yuan2020federated}, variance reduction \cite{gorbunov2020local}, communication compression \cite{basu2019qsparse,haddadpour2020federated,das2020improved}, decentralization \cite{li2019communication,koloskova2020unified}, adaptive and proximal methods \cite{reddi2021adaptive,yuan2020federated_comp}, and resistance to client drift \cite{karimireddy2020scaffold}.
Moshpit SGD can perform multiple local gradient steps before synchronization by design, as shown in Algorithm~\ref{alg:moshpit_local_sgd}.


\subsection{Asynchronous methods}
In the previous subsections, we mostly discussed synchronous distributed methods, since they are more widespread and better studied than asynchronous ones. Mainly, this is because asynchronous methods are more difficult to implement, debug and analyze under general assumptions. However, such methods can be more efficient in terms of using computational resources, which leads to faster wall-clock convergence \cite{assran2020advances}. In recent years, several asynchronous stochastic methods~\cite{recht2011hogwild,zhao2016fast,leblond2017asaga}, methods with no shared memory~\cite{peng2016arock,mishchenko2018delay}, and methods with delayed updates~\cite{agarwal2011distributed,feyzmahdavian2016asynchronous,arjevani2020tight,gorbunov2020linearly} were proposed and analyzed: one can find more details in a recent survey~\cite{assran2020advances}.
Moshpit SGD belongs to this family of asynchronous approaches as well, because the averaging steps happen in smaller groups and can be interleaved with local parameter updates.

\subsection{Distributed Hash Tables}
\label{sect:related_dht}

In this work, we set out to improve distributed averaging with a dynamic matchmaking protocol. Without a central server, this protocol relies on decentralized data structures to organize peers. The main data structure we use is the Distributed Hash Table, or DHT. On a high level, DHT is a distributed fault-tolerant ``dictionary'' that can be accessed by every participant. Each key-value pair is stored on a subset of peers determined by the $\mathrm{hash}$ function of the key.

Each participant has a unique identifier (ID) sampled uniformly from the $\mathrm{hash}$ function output range. When storing a $(key,\ value)$ pair, one must find $k$ peers whose IDs are nearest to $\mathrm{hash}(key)$ according to a chosen metric. After that, the participant requests each of those peers to store $(key,\ value)$. When retrieving a value for a key, one should compute $\mathrm{hash}(key)$, search for peers with IDs nearest to that $\mathrm{hash}$ value and request the value from those peers.

Specific DHT versions, such as Chord~\cite{chord} or Kademlia~\cite{kademlia}, employ different hash types and algorithms for finding nearest peers. For instance, Kademlia DHT sorts peers based on the XOR distance function: $d(x, y) = \mathrm{int}(x \oplus y)$.

In DHT, each participant is directly aware of only a small subset of peers. When storing or retrieving a key, the participant requests additional peers from its neighbors in a semi-greedy search, minimizing the XOR distance until it finds $k$ nearest peers. In Kademlia, nodes form a special navigable graph structure that lets them find nearest peers in at most $\cO(k + \log N)$ requests to other peers, where $N$ is the total number of participants. Due to their scalability and fault-tolerance, DHTs found numerous applications including BitTorrent, Ethereum, I2P and decentralized deep learning~\cite{learning_at_home}.