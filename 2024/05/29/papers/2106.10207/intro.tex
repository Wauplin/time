\section{Introduction}\label{sect:intro}

The deep learning community is becoming increasingly more reliant on transfer learning. In computer vision, pretraining convolutional networks on large image collections such as ImageNet~\cite{imagenet_cvpr09} is the de facto standard for a wide range of applications ranging from object detection~\cite{10.1109/CVPR.2014.81} and semantic segmentation~\cite{7298965} to image classification~\cite{pmlr-v32-donahue14} and even learning perceptual similarity~\cite{johnson2016perceptual}. A growing number of natural language processing systems capitalize on language models with billions of parameters~\cite{bert,albert,roberta,xlmr,gpt3,shoeybi2019megatron} trained on vast unlabeled corpora. Similar trends have emerged in areas such as speech processing~\cite{baevski2020wav2vec}, reinforcement learning~\cite{zhu2021transfer}, and computational biology~\cite{Lu2020.09.04.283929,honda2019smiles}.

Training these models is a notoriously time-consuming and challenging task: it often requires hundreds of high-end GPU servers~\cite{gpt3,megatron2} and would take multiple years on a single device~\cite{lin2020multinode}. Most academic and independent researchers simply cannot afford to train state-of-the-art models from scratch, which slows down scientific progress and practical adoption of deep learning.

Historically, the deep learning community has addressed this problem via ``model hubs'' or ``model zoos'' --- public repositories for pretrained model checkpoints~\cite{tfhub,torchhub,hfhub,dlhub}. These repositories have played a significant role in the democratization of deep learning, allowing everyone to reap the benefits of large-scale training runs conducted by corporations and universities with sufficient resources. However, model hubs are limited to a narrow subset of datasets and tasks that match the interests of model creators. For instance, in natural language processing, it is often difficult to find up-to-date models for more than a handful of languages~\cite{Joshi2020TheSA}. In turn, computer vision hubs rarely feature models trained on drawings, satellite images, 3D renders, microscopy, or any other data that does not resemble ImageNet. As a result, many researchers in these areas can only work on problems for which there are available pretrained models rather than the problems that most need solving.

However, there might be an alternative way to obtain pretrained models: to train these models \emph{collaboratively}. This approach, known as volunteer (or grid) computing, allows many independent parties to combine their computational resources and collectively perform large-scale experiments~\cite{seti_at_home,foldingathome,anderson2004boinc}. The raw compute performance of such collaborations often exceeds that of the fastest supercomputers~\cite{folding_exaflop_2}; however, fully utilizing it can be challenging due to several reasons. First, devices that contribute to collaborative experiments can range from GPU servers and high-end workstations to consumer-grade computers and even smartphones~\cite{Tapparello2016VolunteerCO}. Second, most of these devices use household internet connection with limited bandwidth and low reliability. Third, participants in such projects often donate their hardware part-time, joining and leaving the experiment at will.

While it is theoretically possible to train neural networks on this kind of infrastructure, modern distributed training strategies are only efficient in a narrow range of conditions. For instance, training with Ring All-Reduce~\cite{ringallreduce} works well for identical servers but suffers significant performance penalties from network latency or bandwidth variation~\cite{wagma}. Another technique known as Parameter Server can handle heterogeneous devices at the cost of being less scalable~\cite{ps}. Applying any of these strategies outside their preferred conditions may significantly reduce the training throughput~\cite{shi2018performance}, which makes them difficult to apply in the volatile infrastructure of volunteer computing. This issue is further complicated by the unique limitations of volunteer devices, such as network address translation (NAT), regional access restrictions, or variations in performance.

In this study, we carefully analyze the above challenges and come up with a practical solution for \textbf{D}istribut\textbf{e}d \textbf{D}eep \textbf{L}earning in \textbf{O}pen \textbf{C}ollaborations (DeDLOC). DeDLOC is based on a novel algorithm that adapts to the available hardware in order to maximize the training throughput. Depending on the infrastructure, DeDLOC can recover parameter servers~\cite{ps}, All-Reduce SGD~\cite{sergeev2018horovod}, decentralized SGD~\cite{dp_sgd}, BytePS~\cite{byteps}, or an intermediate strategy that combines all of them. Using this algorithm, we propose a system for collaborative training designed to accommodate a large number of heterogeneous devices with uneven compute, bandwidth, reliability, and network capabilities.

The contributions of our work can be summarized as follows:

\begin{itemize}[leftmargin=*]
    \item We analyze the unique challenges of distributed training in open collaborations and propose a practical recipe for training in these conditions.
    \item We formulate a novel distributed training algorithm that interpolates between traditional strategies to directly maximize the training performance for the available hardware.
    \item We verify the effectiveness of the proposed algorithm and system design for unsupervised pretraining of ALBERT-Large and SwAV under realistic conditions.
    \item We run collaborative training with actual volunteers, achieving competitive results to models trained on hundreds of data center GPUs. We also report insights on the collaborator activity and share the codebase for running similar experiments in the future\footnote{Code and training configurations are available at \href{https://github.com/yandex-research/DeDLOC}{\texttt{github.com/yandex-research/DeDLOC}}}.
\end{itemize}
