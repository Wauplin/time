\section*{Contributions}
\label{sec:contributions}

\vspace{-2pt}
\subsection*{Conceptual}

\textbf{Michael Diskin} derived the optimization problem for adaptive averaging.

\textbf{Max Ryabinin} designed and led the research.

\textbf{Thomas Wolf} initially proposed to run collaborative training with the community participants.

\textbf{Gennady Pekhimenko} supervised the work from the systems design point of view.

\vspace{-2pt}
\subsection*{Technical}

\textbf{Alexey Bukhtiyarov} implemented the core large-batch decentralized optimization procedure. 

\textbf{Dmitry Popov} implemented the support of client mode and auxiliary CPU peers for training.

\textbf{Michael Diskin} implemented and conducted the ALBERT pretraining experiments.

\textbf{Anton Sinitsin and Dmitry Pyrkin} implemented and conducted the SwAV pretraining experiments.

\textbf{Quentin Lhoest} designed and implemented the training data streaming logic.

\textbf{Alexander Borzunov and Lucile Saulnier} proposed and implemented the authentication protocol.

\textbf{Max Ryabinin} provided the initial code for cloud-based ALBERT pretraining.

\textbf{Maxim Kashirin, Denis Mazur, and Ilia Kobelev} implemented the libp2p integration.

\textbf{Max Ryabinin} supervised the development of the project and reviewed the code of contributions.

\vspace{-2pt}
\subsection*{sahajBERT}

\textbf{Michael Diskin, Alexey Bukhtiyarov, and Dmitry Popov} created the notebooks with instructions.

\textbf{Lucile Saulnier} built the tokenizer for sahajBERT and implemented Bengali-specific preprocessing.

\textbf{Michael Diskin, Lucile Saulnier, Max Ryabinin, and Alexander Borzunov} managed the running sahajBERT experiment, monitored its performance, answered the questions of participants, and investigated the occurring errors.

\textbf{Albert Villanova del Moral} implemented and conducted downstream finetuning experiments.

\textbf{Michael Diskin} created the dashboards and implemented continuous reporting of experiment metrics.

\textbf{Alexey Bukhtiyarov} added automatic model state fetching and pushing to Model Hub.

\textbf{Yacine Jernite} helped to find the Neuropark community that was interested in collaborative training. 

\vspace{-2pt}
\subsection*{Writing}

\textbf{Max Ryabinin} composed the initial structure of the paper, wrote its abstract and the introduction.

\textbf{Max Ryabinin, Dmitry Popov, and Alexey Bukhtiyarov} discussed the distributed training, volunteer computing, and federated learning aspects of related work, respectively.

\textbf{Max Ryabinin, Lucile Saulnier, and Yacine Jernite} wrote the conclusion of the work.

\textbf{Michael Diskin} discussed the use of group-based All-Reduce for training in larger collaborations.

\textbf{Michael Diskin} conducted the cost analysis of different distributed training approaches.

\textbf{Maxim Kashirin, Denis Mazur, and Ilia Kobelev} described methods for NAT traversal along with peer-to-peer networking.

\textbf{Max Ryabinin, Michael Diskin, and Anton Sinitsin} outlined decentralized data streaming.

\textbf{Yacine Jernite} assessed the environmental implications of DeDLOC.

\textbf{Gennady Pekhimenko and Thomas Wolf} helped improve the general presentation of the work.

\textbf{Max Ryabinin, Michael Diskin, and Gennady Pekhimenko} edited the final version of the paper.