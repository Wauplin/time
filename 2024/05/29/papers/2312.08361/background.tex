\vspace{-3px}\section{Background: efficient training and inference}\label{sect:background}

There is a wide variety of methods optimizing training and inference for most deep learning workloads. %
Here, we focus on two areas relevant for our analysis: model parallelism and parameter offloading.

\subsection{Model parallelism}\label{sect:background_model_parallel}
Model parallelism is a family of distributed training algorithms that assigns each device to hold a subset of model parameters, run a subset of computations and communicate output activations. \textit{Tensor parallelism} assigns each device to compute a subset of each model layer (e.g., a subset of neurons), then communicate results between each other and proceed to the next layer~\citep{alexnet,model_parallelism_survey1,model_parallelism_survey2}. Each device performs a symmetric computation, applied to a different slice of model weights, which makes tensor parallelism compatible with MPI-based communication. In turn, the main performance overhead of this strategy comes from all-to-all communication (and synchronization) after each layer~\citep{krizhevsky2014oneweirdtrick}.

\textit{Pipeline parallelism} reduces the communication overhead by assigning each device with one or several full layers~\citep{huang2019gpipe,pipedream,pipemare}. During the forward pass, each stage applies its subset of layers to the inputs supplied by the previous stage, then sends the outputs of the last layer to the next stage. For the backward pass, this process is reversed, with each pipeline stage passing the gradients to the same device that previously supplied it with input activations. To better utilize the available devices, the pipeline must process multiple microbatches per step, allowing each stage to run in parallel on a different batch of inputs. Even with optimal execution, some of the pipeline stages will remain idle some of the time~\citep{huang2019gpipe}.

Both of these strategies are actively used for training LLMs. Real-world distributed training systems usually combine multiple forms of parallelism depending on hardware and network type~\citep{megatron2,zero,beyond_data_and_model}.
Tensor parallelism is typically used within a single multi-GPU server or closely interconnected TPU cores~\citep{megatron2,meshtensorflow}. In turn, pipeline parallelism is used to connect multiple servers~\citep{megatron2}. Recent works demonstrate that model parallelism can be used for cost-efficient \textit{pre-training} of LLMs by pooling together idle GPU devices~\citep{varuna,guarantees,bamboo,yuan2022decentralized,ryabinin2023swarm}.

\subsection{Offloading}\label{sect:offload}
Parameter offloading relegates model parameters from accelerator memory to a slower but cheaper storage: typically RAM or SSD~\citep{l2l,zerooffload,zero_ssd}\nocite{accelerate}. When using the model, parameters are loaded to the accelerator just-in-time for computation, one or few layers at a time. In principle, this method allows running large models with a single low-end accelerator as long as there is enough RAM (or SSD) to store the model.

The main drawback of this strategy is having to load and unload through all model parameters for each forward and backward pass, which can be time-consuming. This extra time can be amortized in workloads where model can do a lot of useful computations for each time a parameter is loaded. 
In practice, using offloading to run a single token through the OPT-175B on one GPU in the best-case scenario of hardware and bandwidth\footnote{Specifically, 16-bit parameters, PCIe gen.~4 at 31.5 GB/s (16~lanes), infinite compute and memory bandwidth.} would require 11 seconds per forward pass, or twice that for training. As we show in Section~\ref{sect:experiments}, real-world performance is significantly slower.

\citet{l2l} circumvents this by training with very large batches, and hence, increasing the computation. In turn,~\citet{zerooffload,zero_ssd} reduce the overhead by overlapping communication and computation, that is, doing useful computation for the current layer while waiting for the transfer of the next layer to finish. Some of these systems~\citet{zerooffload} also partition offloaded parameters between devices. However, unlike model-parallel training, distributed offloading still requires each device to compute the full model.
