\section{Introduction}
\label{sec:introduction}


%Ensemble-based uncertainty estimates have been successfully applied to detecting misclassifications, out-of-distribution inputs and adversarial attacks \citep{carlini-detected, gal-adversarial, malinin-rkl-2019} and to active learning~\citep{batchbald}.

%%% THIS NEEDS MORE DETAIL
Ensembles of machine learning models are known to yield improved predictive performance relative to single models~\cite{dietterich2000ensemble}. With the increasing popularity of neural networks, ensemble methods have been rapidly adopted in numerous sub-fields of machine learning~\cite{ashukha2020pitfalls, trust-uncertainty}. More importantly, \cite{deepensemble2017} demonstrated that although single neural networks are often overconfident in their predictions, their ensembles can output reliable uncertainty estimates. Furthermore, ensembles allow \emph{total uncertainty} to be decomposed into \emph{data} and \emph{knowledge uncertainty}\footnote{Data and Knowledge Uncertainty are also known as Aleatoric and Epistemic uncertainty.}. The former is the intrinsic uncertainty due to class overlap and noise inherent in the data, while the latter is the model's uncertainty due to lack of understanding of the test data~\cite{malinin-thesis}. Estimates of \emph{knowledge uncertainty} are often used to detect anomalous and unfamiliar inputs~\cite{batchbald,gal-adversarial, malinin-rkl-2019, malinin-thesis}. Given the increased usage of deep learning for safety-critical applications such as self-driving cars or medical diagnostics, obtaining reliable uncertainty estimates becomes ever more important each year.

However, using ensembles for inference can be computationally prohibitive in certain applications. Obtaining predictions in real time can often be quite involved even for a single model, and the hardware requirements for serving the ensemble of neural networks scale linearly with its size. As a result, over the past several years the area of ensemble distillation has gained increased attention of the research community. Broadly speaking, distillation methods aim to train a single model which can approximate the behavior of the ensemble sufficiently well. 

In the simplest and most frequently used form of distillation \cite{hinton2015distilling}, the student model is trained to capture the average prediction of the ensemble: for example, in case of classification this reduces to KL-divergence between the model and the ensemble mean. While this method allows the student to obtain predictive performance comparable to that of the original ensemble, the information about its distributional properties (in other words, its diversity) is lost in the process. As ensemble-based uncertainty estimation methods often utilize the information about disagreement between its members, such distillation approaches achieve only one of two favorable ensemble properties which we would like to preserve.

% Ensemble Distribution Distillation is a general process. In the original paper we did EnDD via Dirichlet max likelihood. 

%%%% Need a clear contrast with Hydra and so on and a clearer statement
Recently, several works have proposed distillation procedures that capture information about both the mean as well as the distribution of ensemble predictions within a single model~\cite{malinin-endd-2019,hydra,mdd,malinin2020regression}. We will broadly refer to this class of distillation approach as \emph{Ensemble Distribution Distillation} (\Endd). Ensemble Distribution Distillation offers a straightforward way to model the ensemble predictions~\cite{malinin-endd-2019,malinin2020regression}. Outputs of each member are viewed as samples from a higher-order Dirichlet or Normal-Wishart distribution, and the student model attempts to learn the parameters of that distribution. Typically, \Endd is done by maximizing the likelihood the ensemble's output distributions under the Dirichlet or Normal-Wishart Prior. While theoretically sound, for large-scale classification tasks with many classes, gradient-based optimization of this criterion is highly problematic, which limits its usefulness in real-life production scenarios.

In this work, we investigate the poor convergence of models trained with Ensemble Distribution Distillation at scale. We analyze the the Dirichlet log-likelihood criterion and show that it leads to high gradient norm values that affect the optimization procedure. Specifically, if a particular ensemble member's output distribution has most probability mass allocated to a few classes, with the remained spread among a long tail of exponentially less-probable classes, then the gradients associated with the tail-classes will be significantly larger than those associated with high-probability classes. As a result, the model focuses on modelling this distribution of probabilities of tail-classes.

To solve this, we propose to transform the empirical distribution of ensemble member predictions into a \emph{Proxy-target} Dirichlet distribution with the same statistics and to use this distribution as the target during distillation. Furthermore, we show that it is crucial to minimize the \emph{reverse} KL-divergence between the model and the Proxy-Dirichlet, as minimization the \emph{forward} KL-divergence exacerbates the optimizations issues. The proposed training procedure allows the model to converge, mitigating the issue of gradient explosion. We demonstrate this by distribution-distilling ensembles of models trained on both the ImageNet classification and WMT17 English-German language translation datasets, where there are 1000 and 40,000 classes, respectively. On both datasets the distribution-distilled models outperforms models trained from scratch and yield uncertainty estimates competitive this those of the original ensemble. 

Thus, our contributions are as follows:
\begin{itemize}
    \item We analyze the issues of Dirichlet distribution likelihood when applied to a large number of classes and confident predictions
    \item We propose several improvements to the {Ensemble Distribution Distillation} framework, each of them arising from the Dirichlet distribution properties in the context of deep learning
    \item We adapt \emph{Ensemble Distribution Distillation} to auto-regressive models and propose {Sequence Ensemble-Distribution Distillation} (SEnD$^2$)
    \item We examine and propose solutions for a range of technical challenges associated with scaling {Ensemble Distribution Distillation} to large output spaces. 
\end{itemize}.