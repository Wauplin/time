\section{Conclusion}\label{sec:conclusion}

This work examined poor convergence of Ensemble Distribution Distillation when applied to large-scale tasks where the number of classes is very high. We investigated the Dirichlet log-likelihood loss and showed that classes with low probability induce larger gradients than high-probability classes, forcing the model to focus on the distribution of the ensemble tail-class probabilities. We proposed a new training objective which minimizes the reverse KL-divergence to a \emph{Proxy-Dirichlet} target derived from the ensemble. This loss resolves the gradient issues of Ensemble Distribution Distillation, as we demonstrate both theoretically and empirically on the ImageNet and WMT17 En-De datasets containing 1000 and 40,000 classes, respectively. This, this work allows Ensemble-Distribution Distillation to be applied to tasks with arbitrary numbers of classes and complexity, enabling fast ensemble inference through distillation in compute bound, risk-critical applications.
