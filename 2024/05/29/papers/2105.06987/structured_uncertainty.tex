\section{Uncertainty in Structured Prediction via Ensembles}\label{sec:struct-uncertainty}


% Consider models that capture a mapping $\{x_{1},\cdots,x_T\} = \bm{x} \rightarrow \{y_{1},\cdots,y_L\} = \bm{y}$ between a $T$-length sequence of inputs an $L$-length sequence of targets:
% \begin{empheq}{align}
% {\tt P}(\bm{y}| \bm{x}, \bm{\theta}) =&\ \prod_{l=1}^L {\tt P}(y_l | \bm{y}_{<l}, \bm{x}; \bm{\theta})
% \label{eqn:stuct-predict-autoreg}
% \end{empheq}
% In the model above the distribution over each $y_l$ is conditioned on all the previous $\bm{y}_{<l} = \{y_1,\cdots,y_{l-1}\}$, which is called the \emph{context}. These models are typically applied to neural machine translation~\cite{bahdanau2015nmt, attentionisallyouneed}, end-to-end speech recognition \cite{las} and other related tasks. For these models uncertainty estimates can be obtained at two levels - the \emph{token level}, which considers uncertainty in the prediction of a single $y_l$ given a context $\bm{y}_{<l}$ and input $\bm{x}$, and the \emph{sequence level}, which considers the uncertainty of predicting the entire sequence $\bm{y}$ given $\bm{x}$.

% Consider an ensemble of autoregressive models sampled from an approximate posterior ${\tt q}(\bm{\theta})$
% \begin{empheq}{align}
% \{{\tt P}(\bm{y} | \bm{x}; \bm{\theta}^{(m)})\}_{m=1}^M\  \bm{\theta}^{(m)} \sim {\tt q}(\bm{\theta}) \approx {\tt p}(\bm{\theta}|\mathcal{D})
% \label{eqn:stuct-predict-autoreg}
% \end{empheq}
% Given this ensemble we can expressive the predictive posterior either as \emph{sequence-level} Bayesian Model Averaging (BMA) or \emph{token-level} BMA. The former assumes that each sequence $\bm{y}$ has been generated by a \emph{particular} $\bm{\theta}^{(m)}$, as follows:
% \begin{empheq}{align}
% \begin{split}
% {\tt P}(\bm{y}|  \bm{x}, \mathcal{D}) =&\ \mathbb{E}_{{\tt q}(\bm{\theta})}\Big[{\tt P}(\bm{y}| \bm{x},  \bm{\theta})\Big]
%                              \ \ =\ \mathbb{E}_{{\tt q}(\bm{\theta})}\Big[\prod_{l=1}^L {\tt P}(y_l | \bm{y}_{<l},  \bm{x}, \bm{\theta}) \Big]
% \end{split}\label{eqn:expectation-of-products}
% \end{empheq}
% On the other hand, token-level BMA assumes that a particular model $\bm{\theta}^{(m)}$ yields only a particular token $y_l$, and other tokens could haven been generated by other models in the ensemble:
% \begin{empheq}{align}
% \begin{split}
% {\tt P}(\bm{y}|  \bm{x}, \mathcal{D}) =&\  \prod_{l=1}^L {\tt P}(y_l | \bm{y}_{<l},  \bm{x}, \mathcal{D})
%                              =\ \prod_{l=1}^L \mathbb{E}_{{\tt q}(\bm{\theta})}\Big[{\tt P}(y_l | \bm{y}_{<l},  \bm{x}, \bm{\theta})\Big] 
% \end{split}\label{eqn:product-of-expectations}
% \end{empheq}
% Intuitively, it would seem that sequence-level BMA is more appropriate for both model combination and uncertainty estimation in structured prediction. However, it was shown in ~\cite{malinin-structured-2020}, a token-level BMA consistently yields better predictive performance in terms of BLEU and WER, lower negative log-likelihoods on the reference transcriptions and marginally more informative uncertainty estimates than sequence-level BMA. In fact, this result was consistent across all tasks, ensemble sizes and dataset sizes. 

In this section we describe ensemble-based uncertainty estimation for structured prediction. Consider models that capture a mapping $\{\bm{x}_{1},\cdots,\bm{x}_T\} \rightarrow \{y_{1},\cdots,y_L\}$ between a $T$-length sequence of inputs $\{\bm{x}_1,\cdots,\bm{x}_T\} = \bm{x}$ and an $L$-length sequence of targets $\{y_1,\cdots,y_L\} = \bm{y}$, where the targets $\bm{y}$ are auto-regressive:
\begin{empheq}{align}
{\tt P}(\bm{y}| \bm{x}, \bm{\theta}) =&\  \prod_{l=1}^L {\tt P}(y_l | \bm{y}_{<l}, \bm{x}; \bm{\theta})
\label{eqn:stuct-predict-autoreg}
\end{empheq}
In the model above the distribution over each $y_l$ is conditioned on all the previous $\bm{y}_{<l} = \{y_1,\cdots,y_{l-1}\}$, which is called the \emph{context}. These models are typically applied to neural machine translation~\cite{bahdanau2015nmt, attentionisallyouneed}, end-to-end speech recognition \cite{las} and other related tasks. For these models uncertainty estimates can be obtained at two levels - the \emph{token level}, which considers uncertainty in the prediction of a single $y_l$ given a context $\bm{y}_{<l}$ and input $\bm{x}$, and the \emph{sequence level}, which considers the uncertainty of predicting the entire sequence $\bm{y}$ given $\bm{x}$.

Here, we consider ensembles of auto-regressive models sampled from an approximate Bayesian posterior ${\tt q}(\bm{\theta})$:
\begin{empheq}{align}
\big\{ {\tt P}(y_l | \bm{y}_{<l}, \bm{x}; \bm{\theta}^{(m)}) \big\}_{m=1}^M,\  \bm{\theta}^{(m)} \sim {\tt q}(\bm{\theta}) \approx {\tt p}(\bm{\theta}|\mathcal{D})
\label{eqn:stuct-predict-autoreg}
\end{empheq}
Given this ensemble, the predictive posterior can be obtained either as a product of expectations:
\begin{empheq}{align}
{\tt P}_{\tt S-BMA}(\bm{y}|  \bm{x}, \mathcal{D}) =&\  \prod_{l=1}^L {\tt P}(y_l | \bm{y}_{<l},  \bm{x}, \mathcal{D})
                             =\ \prod_{l=1}^L \mathbb{E}_{{\tt q}(\bm{\theta})}\Big[{\tt P}(y_l | \bm{y}_{<l},  \bm{x}, \bm{\theta})\Big] \label{eqn:product-of-expectations} \\
{\tt P}_{\tt T-BMA}(\bm{y}|  \bm{x}, \mathcal{D}) =&\ \mathbb{E}_{{\tt q}(\bm{\theta})}\Big[{\tt P}(\bm{y}| \bm{x},  \bm{\theta})\Big]
                             \ \ =\ \mathbb{E}_{{\tt q}(\bm{\theta})}\Big[\prod_{l=1}^L {\tt P}(y_l | \bm{y}_{<l},  \bm{x}, \bm{\theta}) \Big] \label{eqn:expectation-of-products}
\end{empheq}
As was shown in ~\cite{malinin-structured-2020}, combining the model as a product of expectations consistently yields better BLEU and WER scores as well as better log-likelihoods on the reference transcriptions. Thus, in this work only ensembles combined as a product-of-expectations are considered.