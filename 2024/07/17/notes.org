
* p2p inference.

The idea of a distributed p2p inference system with
a model split into blocks of data, each one split potentially horizontally by shards
where the data flows encrypted from one to the next.

The data from the user is encrypted in a multi-signature transaction so the client
will have control over which peers are allowed to even take part in the contract.

There is a phase where the prices and volumes are worked out so that each
node basically has a larger amount of work planned out for delivery in the future.
This created a derivative futures market for power, network, GPU and CPU where the price of inference
contains all that.

Some applications be willing to accept slower inference for larger amounts of work
, say they are processing a large data set like fine tuning on the Wikipedia data set in bulk batches.
Others might want to pay more for faster inference.

We have a future contract for the delivery of a larger amount of inference in blocks
so that we achieve maximum utility.

** Sharding

split up the input vector horizontally
so that it fits optimally into the smallest GPU.
The size of the shard should fit
into a network packet and flow without hiccup.

** pipe-lining

Each peer sends the results to the next node in the circuit.
we don't want send each result back to a coordinator.
each node buys the results from the previous node,
taking ownership of the data and decryption it.
It can then sell the data to the next.

** Verification

Each inference step will sample a subset of weights of inference that will prove
that the work was done and the data is at hand. This will be requested by the buyer of the data.

** Circuits

Each node takes part in a circuit, a group of nodes that are close to each other in the network
that form the ability to deliver the entire inference chain.
This circuit will feed the results forward along the chain.
Each node will be responsible for validating the results of the previous node.
The Sharding means that you can have multiple parallel processes for each full inference step.

** Pricing

Each node buys the results from the previous nodes and sells
them to the next at a higher value.
each block of inference for each model has a different price and demand.

** ZKP

The zero knowledge proofs for each block can be mined, constructed out of
knowledge of those blocks, that will create a formula that is calculated along side
the inference itself, creating a checksum of sorts or interactive validation
so that the buyer can confirm the work was done and the data is valid.

** queues

we construct queues to move the data efficiently between network cards and the GPU using a pipeline that
delivers the data just in time to the GPU. This is based on network latency, caching, pipelines.

** IREE/MLIR

using the MLIR compiler we can compile the models into programs to run on different hardware.

** Splitting by token position.

We can also further specialize the network by splitting the results up by which pass.
Currently the system sends the output of the last block to first block for the next token.
We can imagine that a miner might specialize in the first token or the Nth token, or
specialize in the value of a token. This can be good for function calling inferences that
look at data after say 100 tokens . We can imagine that the caching of the data
will be more optimal so that cache lines will be more stable for different steps of the inference.


** Long term commitment.

By creating these future contracts and paying miners for blocks of work with risk of losing a large amount
for cheating, we can reduce the risk. 


** idea 1

1. Clients wants to process N requests at the best price, in a time period X.
2. They would escrow that money to smart contract.
3. miners would form squads to bid on that contract, submitting work samples.
   they would bid on buying block N at price P and selling results for N+1 and price P +1.
   if the miner is able to optimize that block and do it faster/cheaper they can bid less cost or time and win contract.
4. client would pick the best squad based on past performance,
   work samples, and price.
5. each node would buy the inputs for processing, decrypt them, process them,
   produce a smaller checksum, 1 float per layer,  and sell to next node, passing on the checksum.
6. checksums will be aggregated, and published, with the hash of the input, inputhash + checksums
7. if the client is not happy with the output, they can flag that transaction, and another squad can
   re-run the process, and decrypt the input with clients permission.
8. if there is a mismatch then you would get confirmations, and then the bad work would be flagged.
   the confirmations would have to be done by a third party. 
   
** idea truebit

1. Clients wants to process N requests at fixed prices, in a time period X.
2. they deposit gas fees into escrow.
3. blockchain assigns job to squad randomly.
4. squad outputs good and bad values.
5. validator checks those outputs and say pass or fail, if fail goes to arbitrage
6. arbitrage : squad has to validate the work, secret is revealed, the loser pays legal fees
   both challegner and solver has to commit to computation steps. challenger checks subset of steps.
   challenger has to prove the solver is wrong, (interactive zkp).
   narrowing down to smaller and smaller problem (bisection).
   
   
